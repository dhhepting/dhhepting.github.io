11h38:
  talks:
  - persid: Daryl Hepting
    msg: >-
      Good morning Good morning.
      Someone's mic on in the background here.
      So I got tripped up a little bit by the zoom schedule meeting
      interface I
      What did I do? I tried to I got into the the app to schedule it
      and then I realized I needed to go into the the website to do it
      because the interfaces are different.
11h39:
  talks:
  - persid: Daryl Hepting
    msg: >-
      So I was changing the settings yesterday because I realized that
      in office hours people weren't I didn't notice that there was a
      person in the waiting room. I thought I had a sound on for that.
11h40:
  talks:
  - persid: Daryl Hepting
    msg: >-
      Anyway, so now I'm
      now I've gotten everyone going to the waiting room and sounds on
      when they both enter the waiting room and enter the meeting. So
      there is no more fine grained control than that.
      It's interesting when I start out in zoom that if I go to the
      zoom.us page, it says where the
11h41:
  talks:
  - persid: Daryl Hepting
    msg: >-
      here's a good one. Let's share that.
      share the screen here for you.
11h42:
  talks:
  - persid: Daryl Hepting
    msg: >-
      Okay
11h45:
  talks:
  - persid: Daryl Hepting
    msg: >-
      so I think these are the same videos here but I'll post the link
      into the chat for everyone.
11h46:
  talks:
  - persid: Daryl Hepting
    msg: >-
      So Can everyone hear those bells ringing it just me
11h47:
  talks:
  - persid: Daryl Hepting
    msg: >-
      Okay, so now we've done the Explore part of the project and the
      last part is going to be materialized which will, which is going
      to cover the
      some testing and improve improvement. So unnecessarily realize a
      very highly functioning prototype of a redesigned website, but
      we can evaluate our two or your two for that too low fidelity
      prototypes. And then
11h48:
  talks:
  - persid: Daryl Hepting
    msg: >-
      and then consider the results of those evaluations and then
      propose and refined the design based on this type of low
      fidelity prototypes and
11h49:
  talks:
  - persid: Daryl Hepting
    msg: >-
      and other ideas you've, you've come across in the process of
      evaluating the low fidelity prototypes. So how do we evaluate
      low fidelity prototypes? Which are paper and pencil drawings,
      per se, primarily.
      So we're just asking people to imagine. So it's like, you see in
      that video, I encourage you to watch that.
      And kind of just lays out how the you might test a feature. So
      he, it to test your gives the participant or the subject? a
      direction of what to do? Pardon me? Oh, yes, of what what the
      goal is to find the answer to some questions on the website, and
      not how to do it. So don't use this feature or that feature. In
      particular, the idea is that you say, here's what you'd like to
      we'd like you, with your to accomplish with this interface. So
      please go ahead and tell me Show me Tell me and show me how you
11h50:
  talks:
  - persid: Daryl Hepting
    msg: >-
      would do it. So instead of having clicking on buttons on a
      webpage, that actually go to different things and invoke
      different functionality, you can say I would press the Next
      button or press whatever button it is. So if I wanted to
      schedule a meeting, I press the schedule button for example, and
      then if that leads me to another screen, another sketch that I
      can see what happens there and so on.
11h51:
  talks:
  - persid: Daryl Hepting
    msg: >-
      So in this time of COVID-19, and our remote configuration the
      semester or at this time, so we'll we'll need to do need to
      adapt our testing our low fidelity prototype evaluation to, to
      zoom or other communication. Medium media.
11h52:
  talks:
  - persid: Daryl Hepting
    msg: >-
      My coffee was behind the screen on my laptop here. That's why I
      tilted the camera.
11h53:
  talks:
  - persid: Daryl Hepting
    msg: >-
      Okay, so what I did on your courses
      is I created a group 16 groups of three, which works out well,
      we have 48 students in class between our two sections.
11h55:
  talks:
  - persid: Daryl Hepting
    msg: >-
      Okay, so here's under p3, should will say materialize the third
      portion of 10, third installment of the project for 10 marks. So
      add these testing groups set up so that each group has three
      people in it. Now I'll just show you the list. So you can go and
      click on people to communicate with them. If you know who's who,
      that's great if you don't, they've got the contact information
      here. So I have a, I wrote a script to make sure to make sure
      that I didn't take people from more than one project group, or
11h56:
  talks:
  - persid: Daryl Hepting
    msg: >-
      pardon me more than one person from any project group and put
      them into a testing group. So the idea is that three people in
      the testing group, each testing group are in different projects.
      So they can
      see you're not testing your own interface with somebody from
      your own group. So if somehow I made an error and I had a script
      to do the to make up the new groups, which I think is working
      correctly but I had to manually transcribe them into your
      courses so that might have introduced and here so that makes
      sense a condition that everybody's from a different project
      group inside a testing group
11h57:
  talks:
  - persid: Daryl Hepting
    msg: >-
      so
      I realized that I have the camera uncovered but the video isn't
      started I don't have the video turned on
11h58:
  talks:
  - persid: Daryl Hepting
    msg: >-
      started showing the those splash screens from zoom about how
      they're number one and all these characters according to all
      these different measures, but I could still see there's room for
      improvement in the interface for example
      the notification the sound notification
11h59:
  talks:
  - persid: Daryl Hepting
    msg: >-
      there's no way to I didn't see a way to say notify me assemblies
      in a waiting room. But don't notify me when somebody enters the
      room. See they're either on fairy thing or off everything
      I did. Anyway
12h01:
  talks:
  - persid: Daryl Hepting
    msg: >-
      Am I doing Okay, so now you can see me right she was me looking.
      However, I look. Now, let me see if I can share my iPad.
12h04:
  talks:
  - persid: Daryl Hepting
    msg: >-
      Okay that was a long way to go for demonstration of new drawing.
12h05:
  talks:
  - persid: Daryl Hepting
    msg: >-
      So,
      if we have so this is in the Assignment Description these names
      of hypothetical group testing group
      Pamela Bob and Sam law so they're all from different project
      groups a few, project one
12h06:
  talks:
  - persid: Daryl Hepting
    msg: >-
      okay.
      So this these are all
      so for draw an arrow between chrome I'm Bob so Kremlin's testing
      her project. So project one, project group one's interface.
      First interface. I don't know what that noise was. So
12h07:
  talks:
  - persid: Daryl Hepting
    msg: >-
      So Pamela
12h08:
  talks:
  - persid: Daryl Hepting
    msg: >-
      same
      you can see that or not
      same dialog box. It's been bugging me for a long time. See if I
      need to send it away for good now.
12h10:
  talks:
  - persid: Daryl Hepting
    msg: >-
      Does that make sense? So everybody in the group, the testing
      group,
12h11:
  talks:
  - persid: Daryl Hepting
    msg: >-
      so, you give her within your group you give a test to each of
      your other group members. And then you
      take the participant for Close your eyes. group members so
      they'll test either their interface day or their interface be
      with you.
12h12:
  talks:
  - persid: Daryl Hepting
    msg: >-
      So if you can schedule two meetings within your test group
      so we have in our example Pamela and Bob
      each test
      I don't know what that was
12h13:
  talks:
  - persid: Daryl Hepting
    msg: >-
      if my cats or listening to the audio on this they'd be going
      crazy taking somebody who's at the door if you don't have a dog
      at this time, you can fill in
12h14:
  talks:
  - persid: Daryl Hepting
    msg: >-
      so so there are three so we make three pairs. People in the
      group there are three pairs, Kremlin Bob, Bob in San Juan, San
      Juan Pamela. And within each pair we do each each in the parent
      tests one of their interfaces with the other. So they do at the
      end is that each member of the testing group will have two
      tests, data from two tests from one test of the interface a and
      one test of interface B. And when you go back to your project
      groups, then you'll have two sets of data or one set of data for
12h15:
  talks:
  - persid: Daryl Hepting
    msg: >-
      each interface
12h16:
  talks:
  - persid: Daryl Hepting
    msg: >-
      per person. So, if you have three group members and you should
      have three sets of data for interface a and three sets of data
      for interface B. And if you have four people in your group, then
      you have four sets of data for interface a and four sets of data
      for your interface B.
      So
      is the sketching helpful at all in any way
12h17:
  talks:
  - persid: Daryl Hepting
    msg: >-
      I have one yes. One Yes, there is and no yeses.
      Anyway, okay, so that makes sense. And I realized I didn't push
      the Document Description, so I'll do that today.
      So
      okay.
12h18:
  talks:
  - persid: Daryl Hepting
    msg: >-
      So kinds of evaluation
      evaluator designs
      are some ways that we can approach
12h19:
  talks:
  - persid: Daryl Hepting
    msg: >-
      so here's the clue. There's two kinds and they both start letter
      Q.
      So any any take any guesses? Oh, wow. straight to the point yes,
      quantitative and qualitative. Let me see if I have a great
      number of blanks.
12h20:
  talks:
  - persid: Daryl Hepting
    msg: >-
      Oh, I missed a blank there.
      I guess I had an extra one there. Quantitative.
12h21:
  talks:
  - persid: Daryl Hepting
    msg: >-
      So, what are the what disease?
      What distinguishes these two?
      Okay, so maybe that's too too easy a question. So, so we have
      this way. We can talk about the data in terms of qualitative or
      quantitative So qualitative is a bit like thinking about how the
      maybe people feel or their experience with an interface. And
      quantitative is more connected to usability, let's say for
      things that we can measure so much more objectively.
12h22:
  talks:
  - persid: Daryl Hepting
    msg: >-
      So if we say those are two different kinds of data,
      what are two different approaches for evaluating?
12h24:
  talks:
  - persid: Daryl Hepting
    msg: >-
      Okay, two other ways to think about evaluation. anyone like to
      buy a ball?
12h25:
  talks:
  - persid: Daryl Hepting
    msg: >-
      Okay, let's do I.
12h26:
  talks:
  - persid: Daryl Hepting
    msg: >-
      Okay.
      This kind of the blank stare again. I was thinking about Having
      some JavaScript function that could plug in on a webpage and
      have the scope more smoothly than you trying sketch it and
12h27:
  talks:
  - persid: Daryl Hepting
    msg: >-
      Carpenter right number of blanks and do allegedly and correctly.
      So what is empirical mean? In this case evaluation.
12h28:
  talks:
  - persid: Daryl Hepting
    msg: >-
      So, okay.
      So we can observe outcomes. Sure. So we're asking
      when we do a study with our within our testing groups, listen,
      we do those studies with our testing groups. Then we're
      collecting empirical data
12h29:
  talks:
  - persid: Daryl Hepting
    msg: >-
      and analytical.
      So if we're doing an analytical approach, and we're using a
      model or some rules and we're not getting direct user input. So
      we're analyzing, analyzing the website or the interface in terms
      of the model that we have.
12h30:
  talks:
  - persid: Daryl Hepting
    msg: >-
      So if we think about
      try that again.
12h31:
  talks:
  - persid: Daryl Hepting
    msg: >-
      So what are so if we did a user test and we gave them Likert
      scale questions or
12h32:
  talks:
  - persid: Daryl Hepting
    msg: >-
      If we measure the response time, so it gives us quantitative
      data. And it's an empirical evaluation.
      So if we get an open ended question, so what did you like about
      the interface? So that online feedback again after the midterm
      that had both that was I was collecting in empirical data, both
      with Likert with quantitative data and qualitative data with
      open ended questions.
12h33:
  talks:
  - persid: Daryl Hepting
    msg: >-
      So what about qualitative analytical?
      Have heuristics
      caught up a lot here. So without using the interface, but
      looking at the appearance on the screen, then we can analyze
      sort of what are good and bad aspects of the interface. And they
      may not become real issues for the user. Because we're not using
      it in context, we're just looking at it
12h34:
  talks:
  - persid: Daryl Hepting
    msg: >-
      outside of use.
      Yeah, so, it's really looking at a theory of what should be
      there and what might cause problems. And,
12h35:
  talks:
  - persid: Daryl Hepting
    msg: >-
      and so the theory might say one thing, but in practice, we have
      another result. So it's not completely
      it's not, we're not necessarily getting the issues are most
      important to users. So it seems to be a glaring error, according
      to Siri may not be so glaring for the user as they're completing
      a particular task with the interface that makes sense.
12h36:
  talks:
  - persid: Daryl Hepting
    msg: >-
      So and then we can also have an analytical, quantitative
      approach. An example here is what's called GaNS, keystroke level
      model.
      So glam stands for goals, operators, methods and selections. I
      believe KLM is for keystroke level model. Not for the Dutch
      airline.
      I don't have my system on Do Not Disturb. So I'm getting I
      messages.
12h37:
  talks:
  - persid: Daryl Hepting
    msg: >-
      So if I have a display plugged in, then it automatically goes to
      do not disturb but
      not Oh, here we go, do not disturb
      and then you can't see that right you cannot, you can still I
      see the iPad screen. Okay. So keystroke level model is basically
      we count keystrokes, for activating different things in the
      interface. So, key presses certain number of seconds mental
      activity or thinking about what to do next is a certain amount
      of time. And switching between a mouse and a keyboard or
      keyboard and the mouse is another amount of time. And then
      pointing with a mouse and clicking it's an amount of time. So,
12h38:
  talks:
  - persid: Daryl Hepting
    msg: >-
      the reason I say that amount of time, instead of a particular
      value is because the values aren't as important as,
      as the relative weight. So switching between a keyboard and a
      mouse takes longer than making a key press and a keyboard and,
      and so on. So we can, we can get a hierarchy of we know which is
      more expensive in terms of time. So, if we have an inexperienced
      typist, or an experienced computer user, their time, there
      absolutely times will be different than somebody who's an
      experienced typist. But the same relationship told that it's
      going to be take more time to shift between the keyboard and a
12h39:
  talks:
  - persid: Daryl Hepting
    msg: >-
      mouse, then you stay on the keyboard for everything. If that
      makes sense. So the idea is for valuing two interfaces, then we
      can get a time for our idealized user to use both interfaces
      based on the standard timings. So it won't be that the actual
      value of the time we come up with in our computation is
      meaningful in itself.
12h40:
  talks:
  - persid: Daryl Hepting
    msg: >-
      If we say that interface a is allows the user to complete more
      quickly as an interface B, then that's, that's the important
      takeaway, I think from the results. So just because interface a
      allows the user to do things more quickly than interface B, that
      may not be the only and may not be the reason to select
      interface a or interface D. Because if the user if it's easy to
      make a mistake,
12h41:
  talks:
  - persid: Daryl Hepting
    msg: >-
      on interface a then efficiency and that quick time is less
      appealing. interface B takes a bit longer, but people are able
      to complete the task without errors, or a few, many fewer errors
      than that's assigned to choose interface B. Has anyone heard of
      Fitts law before.
12h42:
  talks:
  - persid: Daryl Hepting
    msg: >-
      So that relates
      the size of the target we're trying to hit. My typing is not
      nothing is going as well as
12h43:
  talks:
  - persid: Daryl Hepting
    msg: >-
      going as well as I imagine them going on a good day.
      So the idea is of Fitts law, there is a relationship between the
      size of the target that we're trying to hit, and the distance we
      are from the target.
      So if the target is only a couple pixels wide on the screen, and
      we're very far away from it, it's gonna take a long time to to
      hit the target accurately. But if the target is very large, and
      we're not so far away from it, well, even if we are a little far
      away from it, it's still easy to hit. So that's an important
      relationship to keep in mind.
12h44:
  talks:
  - persid: Daryl Hepting
    msg: >-
      So an example of a target, it's easy to hit, let's say, on the
      Mac, since I'm a Mac user, primarily. So if I want to hit some,
      I want to get to the menu bar at the top of the screen, I can go
      quickly up to the top to the menu bar. And I don't have to hit
      it exactly, because the mouse is going to the cursor is going to
      stop at the top of the screen. So I can I can imagine the menu
      bar is very, very large. It's very deep, and therefore easy to
      hit. Conversely, when I'm using Microsoft Word, or when I have
12h45:
  talks:
  - persid: Daryl Hepting
    msg: >-
      used Microsoft Word in the past, and I've tried to use the
      interface to grab the vision graphical user interface to grab
      the margin indicator or margin control. And that's that is
      literally a couple pixels wide. And that's very hard to pick up.
12h46:
  talks:
  - persid: Daryl Hepting
    msg: >-
      So that's why so in that way, it takes a lot even if I'm close
      to it. That's a small target that's hard to yet so that
      increases the time taken. So we can input those that data into
      our calculation into our model and get some more detailed
      results.
      That makes sense. I see where The time here so I have an office
      hour too if you have questions, any questions before we go?
12h47:
  talks:
  - persid: Daryl Hepting
    msg: >-
      Okay, so thanks for today.
      And we'll see you on Tuesday, if not during office hours and
      then also you can reach me online as well. Okay, thanks again
      for today and have a good weekend. Bye
